{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=[\"008770\",\"336260\",\"009830\",\n",
    "      \"086520\",\"011930\",\"018000\",\n",
    "      \"035720\",\"035420\",\"036570\",\n",
    "      \"018260\",\"005930\",\"000660\",\n",
    "      \"011790\",\"014680\",\"000990\",\n",
    "      \"023530\",\"004170\",\"028260\",\n",
    "      \"026960\",\"105560\",\"071050\",\"055550\",\n",
    "      \"316140\",\"086790\"]\n",
    "company=[\"호텔신라\",\"두산퓨얼셀\",\"한화솔루션\",\n",
    "      \"에코프로\",\"신성이엔지\",\"유니슨\",\n",
    "      \"카카오\",\"네이버\",\"엔씨소프트\",\n",
    "      \"sds\",\"삼성전자\",\"SK하이닉스\",\n",
    "      \"skc\",\"한솔케미칼\",\"DB하이텍\",\n",
    "      \"롯데쇼핑\",\"신세계\",\"삼성물산\",\n",
    "      \"동서\",\"kb금융\",\"한국금융지주\",\"신한지주\",\n",
    "      \"우리금융지주\",\"하나금융지주\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "acode={\"호텔신라\":\"008770\",\"두산퓨얼셀\":\"336260\",\"한화솔루션\":\"009830\",\n",
    "      \"에코프로\":\"086520\",\"신성이엔지\":\"011930\",\"유니슨\":\"018000\",\n",
    "      \"카카오\":\"035720\",\"네이버\":\"035420\",\"엔씨소프트\":\"036570\",\n",
    "      \"sds\":\"018260\",\"삼성전자\":\"005930\",\"SK하이닉스\":\"000660\",\n",
    "      \"skc\":\"011790\",\"한솔케미칼\":\"014680\",\"DB하이텍\":\"000990\",\n",
    "      \"롯데쇼핑\":\"023530\",\"신세계\":\"004170\",\"삼성물산\":\"028260\",\n",
    "      \"동서\":\"026960\",\"kb금융\":\"105560\",\"한국금융지주\":\"071050\",\"신한지주\":\"055550\",\n",
    "      \"우리금융지주\":\"316140\",\"하나금융지주\":\"086790\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_URL = \"https://gall.dcinside.com/board/lists?id=neostock&s_type=search_subject_memo&s_keyword=하나금융지주\"\n",
    "a = '/board/lists?id=neostock&s_type=search_subject_memo&s_keyword=하나금융지주'\n",
    "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36'}\n",
    "response = requests.get(BASE_URL,headers=headers)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "items = soup.find('div',{'class':\"bottom_paging_box\"})\n",
    "\n",
    "#리스트 만들기 url\n",
    "url_list=[a]\n",
    "for i in items.find_all('a'):\n",
    "    url_url = i['href']\n",
    "\n",
    "    url_list.append(url_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['DC']\n",
    "collection = db['DC_title_crawl']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "page=30\n",
    "\n",
    "\n",
    "        \n",
    "def title_craw(ls):\n",
    "\n",
    "    \n",
    "    date_stop = \"2021-08-19\"\n",
    "    seen_date = db.DC_title_crawl.find({\"code\":acode}).sort('date', -1).limit(1)\n",
    "    \n",
    "    seen_date = list(seen_date)\n",
    "    if not seen_date:\n",
    "        \n",
    "        seen_date = \"2021.08.19 00:00\"\n",
    "\n",
    "    for i in range(0,len(ls)-1):\n",
    "\n",
    "        response = requests.get('https://gall.dcinside.com' + str(ls[i]),headers=headers)\n",
    "        soup = BeautifulSoup(response.content,'html.parser')\n",
    "        items = soup.find_all(\"table\",{'class':'gall_list'})\n",
    "        contents = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    for j in contents:\n",
    "\n",
    "        if j.find('td',{'class':'gall_writer ub-writer'}).text=='운영자':\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            #new_dict={}\n",
    "            #제목\n",
    "           # title = j.find('a').text\n",
    "           # Title.append(title)\n",
    "\n",
    "\n",
    "        #날짜\n",
    "            date_tag = j.find('td',{'class':'gall_date'})\n",
    "\n",
    "            date_dict = date_tag.attrs\n",
    "\n",
    "\n",
    "            if date_dict['title'][:10] <= date_stop:\n",
    "                TF = True\n",
    "                return -1\n",
    "            else:\n",
    "\n",
    "               # Date.append(date_dict['title'])\n",
    "\n",
    "\n",
    "\n",
    "                #제목\n",
    "                title = j.find('a').text\n",
    "               # Title.append(title)\n",
    "\n",
    "\n",
    "\n",
    "                            #추천수\n",
    "                recommend_tag = j.find('td', class_='gall_recommend')\n",
    "                recommend = recommend_tag.text\n",
    "                #Rec.append(recommend)\n",
    "\n",
    "                            #조회수\n",
    "\n",
    "                views_tag = j.find('td', class_='gall_count')\n",
    "                views = views_tag.text\n",
    "                #View.append(views)\n",
    "                \n",
    "                put_data = {\n",
    "                                 'code':\"086790\",\n",
    "                                 'date':date_dict['title'],\n",
    "                                  'title':title,\n",
    "                                  'Rec':recommend,\n",
    "                                  'View':views           }\n",
    "\n",
    "                db.DC_title_crawl.insert_one(put_data)\n",
    "                \n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    \n",
    "  \n",
    "    title_craw(url_list)\n",
    "\n",
    "    k = 0\n",
    "\n",
    "\n",
    "    while k< page :\n",
    "        response = requests.get('https://gall.dcinside.com' + str(url_list[-1]),headers=headers)\n",
    "        soup = BeautifulSoup(response.content,'html.parser')\n",
    "        items = soup.find('div',{'class':\"bottom_paging_box\"})\n",
    "        url_list=[str(url_list[-1])]\n",
    "\n",
    "        for i in items.find_all('a'):\n",
    "            url_url = i['href']\n",
    "            url_list.append(url_url)\n",
    "        url_list.pop(1) \n",
    "\n",
    "\n",
    "        if title_craw(url_list)==-1:\n",
    "            break\n",
    "        else:\n",
    "            k=k+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
